# -*- coding: utf-8 -*-
"""Abnormality SGD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q2XgFC4t7yBjUuVDyVx1DQULNdzU5BIW
"""


from __future__ import absolute_import, division, print_function, unicode_literals

import math

import numpy as np
import torch
from torch import optim

import torch.nn as nn
import pickle
from tqdm import tqdm

from train_test_loaders import getLoader_CIFAR_LENET
from ratefunction import rate_function_single_value, eval_inverse_rate_at_lambda_signed, eval_rate_at_lambda_signed
from train_eval import get_loss_grad, get_loss, jensen_val_grad, get_data_set_grad
from utils import list_files_in_directory, ensure_directory_exists
import argparse
from scipy.spatial import distance
import os
import sys

base_folder = "./"

"""Data"""


dataset_name = "CIFAR10"



parser = argparse.ArgumentParser()
parser.add_argument("-m", "--MODEL", help="MODEL", default= "MLP", type=str)
parser.add_argument("-b", "--BATCH_SIZE", help="BATCH_SIZE", default= 2500, type=int)
parser.add_argument("-i", "--N_ITERS", help="N_ITERS", default= 5000, type=int)
parser.add_argument("-s", "--N_STEPS", help="N_STEPS", default= 50, type=int)
parser.add_argument("-w", "--WEIGHT_DECAY", help="WEIGHT_DECAY", default= 0.0, type=float)


args = parser.parse_args()

network = args.MODEL
n_models = 10
TRAIN_SIZE = 0
TEST_SIZE = 0
BATCH_SIZE = args.BATCH_SIZE
# Hyper-Parameters
RANDOM_SEED = 2147483647
LEARNING_RATE = 0.001
N_ITERS = args.N_ITERS
N_STEPS = args.N_STEPS
IMG_SIZE = 32
N_CLASSES = 10
WEIGHT_DECAY = args.WEIGHT_DECAY

name_model=f"{network}_{dataset_name}_{TRAIN_SIZE}_{TEST_SIZE}_{BATCH_SIZE}_{WEIGHT_DECAY}"

# setup devices
torch.manual_seed(RANDOM_SEED)
if torch.cuda.is_available():
    device = torch.device("cuda:0")
    torch.cuda.manual_seed(RANDOM_SEED)
elif torch.backends.mps.is_available():
    device = torch.device("mps")
    torch.cuda.manual_seed(RANDOM_SEED)
else:
    device = torch.device("cpu")

"""# Train"""

train_loader_batch, _ = getLoader_CIFAR_LENET(TRAIN_SIZE,TEST_SIZE, BATCH_SIZE, BATCH_SIZE, RANDOM_SEED)
train_loader_full, test_loader_full = getLoader_CIFAR_LENET(TRAIN_SIZE,TEST_SIZE, 1000, 1000, RANDOM_SEED)

torch.manual_seed(RANDOM_SEED)


# Example usage
directory_path = f'./models/{name_model}/'
files = list_files_in_directory(directory_path)

grad_directory_path = f'./gradients/{name_model}/'
ensure_directory_exists(grad_directory_path)

g_cuda = torch.Generator(device='cpu')
g_cuda.manual_seed(RANDOM_SEED)


criterion = nn.CrossEntropyLoss()
criterion_nr = nn.CrossEntropyLoss(reduction="none")  # supervised classification loss

with open(directory_path + files[0], "rb") as handle:
    model = pickle.load(handle)

optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)

metrics = {
    'iter': [],
    'lamb_rate_D': [],
    'lamb_rate_B': [],
    'J_D': [],
    'J_B': [],
    'rate_D': [],
    'rate_B': [],
    'inv_D': [],
    'inv_D_approx': [],
    'inv_B': [],
    'inv_B_approx': [],
    'L': [],
    'Lhat': [],
    'Lbatch': [],
    'variance': [],
    'cosine_Iinv': [],
    'cosine_Alpha': [],
    'cosine_Lhat_Lbatch': [],
    'gradients_L_norm': [],
    'gradients_Lhat_norm': [],
    'gradients_Lbatch_norm': [],
    'gradients_Iinv_D_norm': [],
    'gradients_Iinv_batch_norm': [],
    'gradients_alpha_D_norm': [],
    'gradients_alpha_batch_norm': []
}

data_iter = iter(train_loader_batch)
iters_per_epoch = len(data_iter)
aux_loss = 1

tq = tqdm(range(N_ITERS))


for it in tq:

    # Get inputs and targets. If loader is exhausted, reinitialize.
    try:
        inputs, target = next(data_iter)
    except StopIteration:
        # StopIteration is thrown if dataset ends
        # reinitialize data loader
        data_iter = iter(train_loader_batch)
        inputs, target = next(data_iter)


    if it % N_STEPS == 0 and it != 0:
        with open(directory_path + f"model_{int(it)}.pickle", "rb") as handle:
            model = pickle.load(handle)
        optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)

        optimizer.zero_grad()
        # Forward pass
        L = get_data_set_grad(device, model, test_loader_full)

        gradients_L=[]
        for name, param in model.named_parameters():
            if param.grad is not None:
                gradients_L.append(param.grad.clone().flatten())

        gradients_L = torch.cat(gradients_L,dim=0).detach().cpu().numpy()

        metrics['gradients_L_norm'].append(np.linalg.norm(gradients_L))

        optimizer.zero_grad()
        Lhat = get_data_set_grad(device, model, train_loader_full)

        gradients_Lhat=[]
        for name, param in model.named_parameters():
            if param.grad is not None:
                gradients_Lhat.append(param.grad.clone().flatten())

        gradients_Lhat = torch.cat(gradients_Lhat,dim=0).detach().cpu().numpy()
        metrics['gradients_Lhat_norm'].append(np.linalg.norm(gradients_Lhat))

        optimizer.zero_grad()
        # Move data to device
        inputs = inputs.to(device)
        target = target.to(device)
        Lbatch = criterion(model(inputs), target)
        Lbatch.backward()
        Lbatch = Lbatch.detach().cpu().numpy()

        gradients_Lbatch=[]
        for name, param in model.named_parameters():
            if param.grad is not None:
                gradients_Lbatch.append(param.grad.clone().flatten())

        gradients_Lbatch = torch.cat(gradients_Lbatch,dim=0).detach().cpu().numpy()
        metrics['gradients_Lbatch_norm'].append(np.linalg.norm(gradients_Lbatch))

        cosine_Lhat_Lbatch=1-distance.cosine(gradients_Lhat, gradients_Lbatch)
        metrics['cosine_Lhat_Lbatch'].append(cosine_Lhat_Lbatch)

        inputs = None
        target = None


        metrics['iter'].append(it)
        metrics['L'].append(L)
        metrics['Lhat'].append(Lhat)
        metrics['Lbatch'].append(Lbatch)

        with torch.no_grad():
            #Model.eval
            losses_nograd = get_loss(device, model, test_loader_full)
            metrics['variance'].append(torch.var(losses_nograd).detach().cpu().numpy())

            rate_val, lamb_rate, J_D = rate_function_single_value(losses_nograd, torch.tensor(L - Lhat, dtype=torch.float32).to(device), device)
            metrics['rate_D'].append(rate_val)
            metrics['lamb_rate_D'].append(lamb_rate)
            metrics['J_D'].append(J_D)

            rate_val = torch.tensor(rate_val, dtype=torch.float32).to(device)
            lamb_rate = torch.tensor(lamb_rate, dtype=torch.float32).to(device)
            inv_D = eval_inverse_rate_at_lambda_signed(losses_nograd, lamb_rate, rate_val, device)
            metrics['inv_D'].append(inv_D.detach().cpu().numpy())

            rate_val_batch, lamb_rate_batch, J_batch = rate_function_single_value(losses_nograd, torch.tensor(L - Lbatch, dtype=torch.float32).to(device), device)
            metrics['rate_B'].append(rate_val_batch)
            metrics['lamb_rate_B'].append(lamb_rate_batch)
            metrics['J_B'].append(J_batch)

            rate_val_batch = torch.tensor(rate_val_batch, dtype=torch.float32).to(device)
            lamb_rate_batch = torch.tensor(lamb_rate_batch, dtype=torch.float32).to(device)

            inv_B = eval_inverse_rate_at_lambda_signed(losses_nograd, lamb_rate_batch, rate_val_batch, device)
            metrics['inv_B'].append(inv_B.detach().cpu().numpy())

            metrics['inv_D_approx'].append((torch.sign(rate_val)*torch.sqrt_(2*torch.abs(rate_val)*torch.var(losses_nograd))).detach().cpu().numpy())
            metrics['inv_B_approx'].append((torch.sign(rate_val_batch)*torch.sqrt_(2*torch.abs(rate_val_batch)*torch.var(losses_nograd))).detach().cpu().numpy())


        #Grad Iinv(theta,alpha_D)

        optimizer.zero_grad()
        sign_D = torch.sign(rate_val).detach()
        jensen_val_grad(device, model, test_loader_full, sign_D, lamb_rate.detach(), losses_nograd)
        gradients_Iinv_D=[]
        for name, param in model.named_parameters():
            if param.grad is not None:
                gradients_Iinv_D.append(param.grad.clone().flatten())

        gradients_Iinv_D = sign_D.detach().cpu().numpy()*torch.cat(gradients_Iinv_D,dim=0).detach().cpu().numpy() + gradients_L
        metrics['gradients_Iinv_D_norm'].append(np.linalg.norm(gradients_Iinv_D))



        optimizer.zero_grad()
        sign_B = torch.sign(rate_val_batch).detach()
        jensen_val_grad(device, model, test_loader_full, sign_B, lamb_rate_batch.detach(), losses_nograd)
        gradients_Iinv_batch=[]
        for name, param in model.named_parameters():
            if param.grad is not None:
                gradients_Iinv_batch.append(param.grad.clone().flatten())

        gradients_Iinv_batch =  sign_B.detach().cpu().numpy()*torch.cat(gradients_Iinv_batch,dim=0).detach().cpu().numpy() + gradients_L
        metrics['gradients_Iinv_batch_norm'].append(np.linalg.norm(gradients_Iinv_batch))

        cosine_Iinv=1-distance.cosine(gradients_Iinv_D, gradients_Iinv_batch)
        metrics['cosine_Iinv'].append(cosine_Iinv)

        gradients_alpha_D = sign_D.detach().cpu().numpy()*(gradients_L - gradients_Iinv_D - gradients_Lhat)
        gradients_alpha_batch = sign_B.detach().cpu().numpy()*(gradients_L - gradients_Iinv_batch - gradients_Lbatch)
        metrics['gradients_alpha_D_norm'].append(np.linalg.norm(gradients_alpha_D))
        metrics['gradients_alpha_batch_norm'].append(np.linalg.norm(gradients_alpha_batch))

        cosine_Alpha=1-distance.cosine(gradients_alpha_D, gradients_alpha_batch)
        metrics['cosine_Alpha'].append(cosine_Alpha)


        with open(grad_directory_path + f"gradients_L{int(it)}.pickle", "wb") as handle:
            pickle.dump(gradients_L, handle, protocol=pickle.HIGHEST_PROTOCOL)

        with open(grad_directory_path + f"gradients_Lhat{int(it)}.pickle", "wb") as handle:
            pickle.dump(gradients_Lhat, handle, protocol=pickle.HIGHEST_PROTOCOL)

        with open(grad_directory_path + f"gradients_Lbatch{int(it)}.pickle", "wb") as handle:
            pickle.dump(gradients_Lbatch, handle, protocol=pickle.HIGHEST_PROTOCOL)

        with open(grad_directory_path + f"gradients_Iinv_D{int(it)}.pickle", "wb") as handle:
            pickle.dump(gradients_Iinv_D, handle, protocol=pickle.HIGHEST_PROTOCOL)

        with open(grad_directory_path + f"gradients_Iinv_batch{int(it)}.pickle", "wb") as handle:
            pickle.dump(gradients_Iinv_batch, handle, protocol=pickle.HIGHEST_PROTOCOL)

        with open(grad_directory_path + f"gradients_alpha_D{int(it)}.pickle", "wb") as handle:
            pickle.dump(gradients_alpha_D, handle, protocol=pickle.HIGHEST_PROTOCOL)

        with open(grad_directory_path + f"gradients_alpha_batch{int(it)}.pickle", "wb") as handle:
            pickle.dump(gradients_alpha_batch, handle, protocol=pickle.HIGHEST_PROTOCOL)


with open(f'metrics/metrics_{name_model}.pickle', 'wb') as f:
    pickle.dump(metrics, f, pickle.HIGHEST_PROTOCOL)



