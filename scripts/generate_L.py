# -*- coding: utf-8 -*-
"""Abnormality SGD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q2XgFC4t7yBjUuVDyVx1DQULNdzU5BIW
"""


from __future__ import absolute_import, division, print_function, unicode_literals

import math

import numpy as np
import torch
from torch import optim

import torch.nn as nn
import pickle
from tqdm import tqdm

from train_test_loaders import getLoader_CIFAR_LENET
from ratefunction import rate_function_single_value, eval_inverse_rate_at_lambda_signed, eval_rate_at_lambda_signed
from train_eval import get_loss_grad, get_loss
from utils import list_files_in_directory
import argparse
import os
import sys

base_folder = "./"

"""Data"""


dataset_name = "CIFAR10"




parser = argparse.ArgumentParser()
parser.add_argument("-m", "--MODEL", help="MODEL", default= "MLP", type=str)
parser.add_argument("-b", "--BATCH_SIZE", help="BATCH_SIZE", default= 2500, type=int)
parser.add_argument("-i", "--N_ITERS", help="N_ITERS", default= 3000, type=int)
parser.add_argument("-s", "--N_STEPS", help="N_STEPS", default= 30, type=int)
parser.add_argument("-w", "--WEIGHT_DECAY", help="WEIGHT_DECAY", default= 0.0, type=float)

args = parser.parse_args()

network = args.MODEL
TRAIN_SIZE = 0
TEST_SIZE = 0
BATCH_SIZE = args.BATCH_SIZE
# Hyper-Parameters
RANDOM_SEED = 2147483647
LEARNING_RATE = 0.01
N_ITERS = args.N_ITERS
N_STEPS = args.N_STEPS
IMG_SIZE = 32
N_CLASSES = 10
WEIGHT_DECAY = args.WEIGHT_DECAY

name_model=f"{network}_{dataset_name}_{TRAIN_SIZE}_{TEST_SIZE}_{BATCH_SIZE}_{WEIGHT_DECAY}"

# setup devices
torch.manual_seed(RANDOM_SEED)
if torch.cuda.is_available():
    device = torch.device("cuda:0")
    torch.cuda.manual_seed(RANDOM_SEED)
elif torch.backends.mps.is_available():
    device = torch.device("mps")
    torch.cuda.manual_seed(RANDOM_SEED)
else:
    device = torch.device("cpu")

"""# Train"""

train_loader_batch, _ = getLoader_CIFAR_LENET(TRAIN_SIZE,TEST_SIZE, BATCH_SIZE, BATCH_SIZE, RANDOM_SEED)
train_loader_full, test_loader_full = getLoader_CIFAR_LENET(TRAIN_SIZE,TEST_SIZE, 500, 500, RANDOM_SEED)

torch.manual_seed(RANDOM_SEED)


# Example usage
directory_path = f'./models/{name_model}/'
files = list_files_in_directory(directory_path)


g_cuda = torch.Generator(device='cpu')
g_cuda.manual_seed(RANDOM_SEED)


criterion = nn.CrossEntropyLoss().to(device)

criterion_nr = nn.CrossEntropyLoss(reduction="none").to(device)  # supervised classification loss

with open(directory_path + files[0], "rb") as handle:
    model = pickle.load(handle)

optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)
# Initialize metrics for moments and cumulants
metrics = {
    'iter': [],
    'L': [],
    'Lhat': [],
    'Lbatch': [],
    'raw_moments': {'m1': [], 'm2': [], 'm3': [], 'm4': []},
    'central_moments': {'c1': [], 'c2': [], 'c3': [], 'c4': []},
    'cumulants': {'k1': [], 'k2': [], 'k3': [], 'k4': []},
    'central_cumulants': {'ck1': [], 'ck2': [], 'ck3': [], 'ck4': []},
}


def compute_raw_moments(losses):
    """Compute the first four raw moments."""
    m1 = np.mean(losses)
    m2 = np.mean(losses ** 2)
    m3 = np.mean(losses ** 3)
    m4 = np.mean(losses ** 4)
    return m1, m2, m3, m4

def compute_central_moments(losses, m1):
    """Compute the first four central moments."""
    deviations =  m1 - lo
    c1 = np.mean(deviations)  # The first central moment is simply the mean
    c2 = np.mean(deviations ** 2)  # Variance
    c3 = np.mean(deviations ** 3)
    c4 = np.mean(deviations ** 4)
    return c1, c2, c3, c4

def compute_cumulants(c1, c2, c3, c4):
    """Compute the first four cumulants from the central moments."""
    k1 = c1
    k2 = c2
    k3 = c3
    k4 = c4 - 3 * (c2 ** 2)
    return k1, k2, k3, k4


data_iter = iter(train_loader_batch)
iters_per_epoch = len(data_iter)
aux_loss = 1


tq = tqdm(range(N_ITERS))


for it in tq:

    # Get inputs and targets. If loader is exhausted, reinitialize.
    try:
        inputs, target = next(data_iter)
    except StopIteration:
        # StopIteration is thrown if dataset ends
        # reinitialize data loader
        data_iter = iter(train_loader_batch)
        inputs, target = next(data_iter)


    if (it % N_STEPS == 0 and it != 0):
        with open(directory_path + f"model_{int(it)}.pickle", "rb") as handle:
            model = pickle.load(handle)

        model.to(device)
        with torch.no_grad():
                # Forward pass
            test_loss = get_loss(device, model, test_loader_full).detach().cpu().numpy()
            L = np.mean(test_loss)
            train_loss = get_loss(device, model, train_loader_full).detach().cpu().numpy()
            Lhat = np.mean(train_loss)

            # Move data to device
            inputs = inputs.to(device)
            target = target.to(device)
            Lbatch = criterion(model(inputs), target).detach().cpu().numpy()

        # Compute raw moments for the batch
        m1, m2, m3, m4 = compute_raw_moments(test_loss)

        # Compute central moments based on the raw mean (m1)
        c1, c2, c3, c4 = compute_raw_moments(m1-test_loss)

        # Compute cumulants based on the moments
        k1, k2, k3, k4 = compute_cumulants(m1, m2, m3, m4)

        # Compute cumulants based on the central moments
        ck1, ck2, ck3, ck4 = compute_cumulants(c1, c2, c3, c4)

        metrics['iter'].append(it)
        metrics['L'].append(L)
        metrics['Lhat'].append(Lhat)
        metrics['Lbatch'].append(Lbatch)

        metrics['raw_moments']['m1'].append(m1)
        metrics['raw_moments']['m2'].append(m2)
        metrics['raw_moments']['m3'].append(m3)
        metrics['raw_moments']['m4'].append(m4)
        metrics['central_moments']['c1'].append(c1)
        metrics['central_moments']['c2'].append(c2)
        metrics['central_moments']['c3'].append(c3)
        metrics['central_moments']['c4'].append(c4)
        metrics['cumulants']['k1'].append(k1)
        metrics['cumulants']['k2'].append(k2)
        metrics['cumulants']['k3'].append(k3)
        metrics['cumulants']['k4'].append(k4)
        metrics['central_cumulants']['ck1'].append(ck1)
        metrics['central_cumulants']['ck2'].append(ck2)
        metrics['central_cumulants']['ck3'].append(ck3)
        metrics['central_cumulants']['ck4'].append(ck4)



with open(f'metrics/L_metrics_{name_model}.pickle', 'wb') as f:
    pickle.dump(metrics, f, pickle.HIGHEST_PROTOCOL)



