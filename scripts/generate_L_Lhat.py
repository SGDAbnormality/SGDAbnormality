# -*- coding: utf-8 -*-
"""Abnormality SGD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q2XgFC4t7yBjUuVDyVx1DQULNdzU5BIW
"""


from __future__ import absolute_import, division, print_function, unicode_literals

import math

import torch
from torch import optim

import torch.nn as nn
import pickle
from tqdm import tqdm

from train_test_loaders import getLoader_CIFAR_LENET
from ratefunction import rate_function_single_value, eval_inverse_rate_at_lambda_signed, eval_rate_at_lambda_signed
from train_eval import get_loss_grad
from utils import list_files_in_directory
import argparse
import os
import sys

base_folder = "./"

"""Data"""


network = "LeNet5_K5"
dataset_name = "CIFAR10"


parser = argparse.ArgumentParser()
parser.add_argument("-b", "--BATCH_SIZE", help="BATCH_SIZE", default= 50, type=int)

args = parser.parse_args()

TRAIN_SIZE = 0
TEST_SIZE = 0
BATCH_SIZE = args.BATCH_SIZE
# Hyper-Parameters
RANDOM_SEED = 2147483647
LEARNING_RATE = 0.01
N_ITERS = 2000000
IMG_SIZE = 32
N_CLASSES = 10
WEIGHT_DECAY = 0.01

name_model=f"{network}_{dataset_name}_{TRAIN_SIZE}_{TEST_SIZE}_{BATCH_SIZE}"

# setup devices
torch.manual_seed(RANDOM_SEED)
if torch.cuda.is_available():
    device = torch.device("cuda:0")
    torch.cuda.manual_seed(RANDOM_SEED)
elif torch.backends.mps.is_available():
    device = torch.device("mps")
    torch.cuda.manual_seed(RANDOM_SEED)
else:
    device = torch.device("cpu")

"""# Train"""

train_loader_batch, _ = getLoader_CIFAR_LENET(TRAIN_SIZE,TEST_SIZE, BATCH_SIZE, BATCH_SIZE, RANDOM_SEED)
train_loader_full, test_loader_full = getLoader_CIFAR_LENET(TRAIN_SIZE,TEST_SIZE, BATCH_SIZE, BATCH_SIZE, RANDOM_SEED)

torch.manual_seed(RANDOM_SEED)


# Example usage
directory_path = f'./models/{name_model}/'
files = list_files_in_directory(directory_path)


g_cuda = torch.Generator(device='cpu')
g_cuda.manual_seed(RANDOM_SEED)


criterion = nn.CrossEntropyLoss()
criterion_nr = nn.CrossEntropyLoss(reduction="none")  # supervised classification loss

with open(directory_path + files[0], "rb") as handle:
    model = pickle.load(handle)

optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)

metrics = {
    'iter': [],
    'lamb_rate_D': [],
    'lamb_rate_B': [],
    'J_D': [],
    'J_B': [],
    'rate_D': [],
    'rate_B': [],
    'inv_D': [],
    'inv_D_approx': [],
    'inv_B': [],
    'inv_B_approx': [],
    'L': [],
    'Lhat': [],
    'Lbatch': [],
    'variance': [],
    'cosine_Iinv': [],
    'cosine_Alpha': [],
    'cosine_Lhat_Lbatch': [],
    'gradientNorm_L': [],
    'gradientNorm_Iinv': [],
    'gradientNorm_alpha': []
}

data_iter = iter(train_loader_batch)
iters_per_epoch = len(data_iter)
aux_loss = 1

total = 10*(len(files)-1)
n_models = 100
steps = math.ceil((total/n_models)/10) * 10
tq = tqdm(range(total))

#tq = tqdm(range(20))

for it in tq:

    # Get inputs and targets. If loader is exhausted, reinitialize.
    try:
        inputs, target = next(data_iter)
    except StopIteration:
        # StopIteration is thrown if dataset ends
        # reinitialize data loader
        data_iter = iter(train_loader_batch)
        inputs, target = next(data_iter)

    # Move data to device
    inputs = inputs.to(device)
    target = target.to(device)

    if it % steps == 0:
        with open(directory_path + f"model_{int(it)}.pickle", "rb") as handle:
            model = pickle.load(handle)
        optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)

        optimizer.zero_grad()
        # Forward pass
        losses = get_loss_grad(device, model, test_loader_full)
        L = torch.mean(losses)
        Lhat = torch.mean(get_loss_grad(device, model, train_loader_full))
        Lbatch = criterion(model(inputs), target)
        metrics['iter'].append(it)
        metrics['L'].append(L.detach().cpu().numpy())
        metrics['Lhat'].append(Lhat.detach().cpu().numpy())
        metrics['Lbatch'].append(Lbatch.detach().cpu().numpy())



with open(f'metrics/L_metrics_{name_model}.pickle', 'wb') as f:
    pickle.dump(metrics, f, pickle.HIGHEST_PROTOCOL)



