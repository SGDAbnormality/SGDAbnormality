# -*- coding: utf-8 -*-
"""Abnormality SGD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q2XgFC4t7yBjUuVDyVx1DQULNdzU5BIW
"""

from __future__ import absolute_import, division, print_function, unicode_literals
import numpy as np
import torch
from torchvision import transforms
import torchvision

import torch.nn as nn
import pickle
import os
import sys

sys.path.append(".")
from ratefunction import LeNet5, rate_function
from train_eval import train, eval, get_loss_samples

base_folder = "./Fig2"

"""Data"""


TRAIN_SIZE = 1000
BATCH_SIZE_TEST = 500
RANDOM_SEED = 2147483647
LEARNING_RATE = 0.001
N_ITERS = 5000

# setup devices
torch.manual_seed(RANDOM_SEED)
if torch.cuda.is_available():
    device = torch.device("cuda:0")
    torch.cuda.manual_seed(RANDOM_SEED)
else:
    device = torch.device("cpu")

transform = transforms.Compose(
    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]
)

train_dataset = torchvision.datasets.MNIST(
    root="./data/", train=True, download=True, transform=transform
)

test_dataset = torchvision.datasets.MNIST(
    root="./data/", train=False, download=True, transform=transform
)

train_dataset_inputs = []
train_dataset_targets = []


test_dataset_inputs = []
test_dataset_targets = []
skip = True
for input, target in torch.utils.data.DataLoader(
    train_dataset, batch_size=TRAIN_SIZE, shuffle=False
):
    if skip:
        train_dataset_inputs.append(input)
        train_dataset_targets.append(target)
        skip = False
        continue
    test_dataset_inputs.append(input)
    test_dataset_targets.append(target)

for input, target in torch.utils.data.DataLoader(
    test_dataset, batch_size=BATCH_SIZE_TEST, shuffle=False
):
    test_dataset_inputs.append(input)
    test_dataset_targets.append(target)


test_dataset_inputs = torch.concatenate(test_dataset_inputs)
test_dataset_targets = torch.concatenate(test_dataset_targets)

train_dataset = torch.utils.data.TensorDataset(
    train_dataset_inputs[0], train_dataset_targets[0]
)

test_dataset = torch.utils.data.TensorDataset(test_dataset_inputs, test_dataset_targets)
test_loader = torch.utils.data.DataLoader(
    test_dataset, batch_size=BATCH_SIZE_TEST, shuffle=False
)


weight_decays = [0.0, 0.025]
batch_sizes = [50, 50]

labels = ["SGD-50-0", "SGD-50-1"]

"""# Train"""

models = []
torch.manual_seed(RANDOM_SEED)
models.append(LeNet5().to(device))

torch.manual_seed(RANDOM_SEED)
models.append(LeNet5().to(device))

# Check if models are trained at base folder
if os.path.exists(base_folder + f"/models/{labels[0]}.pickle"):
    with open(base_folder + f"/models/{labels[0]}.pickle", "rb") as handle:
        models[0] = pickle.load(handle)
    with open(base_folder + f"/models/{labels[1]}.pickle", "rb") as handle:
        models[1] = pickle.load(handle)
else:
    for i in range(len(models)):
        g_cuda = torch.Generator(device="cpu")
        g_cuda.manual_seed(RANDOM_SEED)
        loader = torch.utils.data.DataLoader(
            dataset=train_dataset,
            batch_size=batch_sizes[i],
            generator=g_cuda,
            shuffle=True,
        )
        train(
            models[i],
            loader,
            LEARNING_RATE,
            N_ITERS,
            weight_decays[i],
            device,
            nn.CrossEntropyLoss(),
        )
        with open(base_folder + f"/models/{labels[i]}.pickle", "wb") as handle:
            pickle.dump(models[i], handle, protocol=pickle.HIGHEST_PROTOCOL)

L_test = []
if not os.path.exists(base_folder + f"/Ltest/L_test_{labels[0]}.txt"):
    for i in range(len(models)):
        L_test.append(eval(device, models[i], test_loader, nn.CrossEntropyLoss())[2])
        print("Test NLL ", labels[i], ": ", L_test[-1])
        np.savetxt(base_folder + f"/Ltest/L_test_{labels[i]}.txt", np.array([L_test[-1]]))

"""# Sampling $\alpha$ for a given model using data sets of size $batch\_size$"""

N_SAMPLES = 20000
batch_sizes = [50, 500]
names = ["BatchLosses50", "BatchLosses500"]


if not os.path.exists(base_folder + f"/samples/{names[0]}-Models0.txt"):
    for k in range(len(batch_sizes)):
        batch_losses_vec = []

        for i in range(len(models)):
            g_cuda = torch.Generator(device="cpu")
            g_cuda.manual_seed(RANDOM_SEED)
            slices_loader = torch.utils.data.DataLoader(
                test_dataset, batch_size=batch_sizes[k], generator=g_cuda, shuffle=True
            )

            batch_losses = get_loss_samples(
                device, models[i], slices_loader, N_SAMPLES, L_test[i]
            )

            np.savetxt(base_folder + f"/samples/{names[k]}-Models{i}.txt", batch_losses)


batch_losses_50 = [np.loadtxt("Fig2\samples\BatchLosses50-Models0.txt"), np.loadtxt("Fig2\samples\BatchLosses50-Models1.txt")]
batch_losses_500  = [np.loadtxt("Fig2\samples\BatchLosses500-Models0.txt"), np.loadtxt("Fig2\samples\BatchLosses500-Models1.txt")]

Ltest0 = np.loadtxt("Fig2\Ltest\L_test_SGD-50-0.txt")
Ltest1 = np.loadtxt("Fig2\Ltest\L_test_SGD-50-1.txt")
Ltest = [Ltest0, Ltest1]

rate_functions = []
for i in range(len(models)):
    rate_functions = np.array(rate_function(models[i].to(device), Ltest[i] - batch_losses_50[i], device, test_loader))
    np.savetxt(base_folder + f"/rates/50-Models{i}.txt", rate_functions)
    rate_functions = np.array(rate_function(models[i].to(device), Ltest[i] - batch_losses_500[i], device, test_loader))
    np.savetxt(base_folder + f"/rates/500-Models{i}.txt", rate_functions)
