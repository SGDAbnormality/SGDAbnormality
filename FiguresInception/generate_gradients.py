# -*- coding: utf-8 -*-
"""Abnormality SGD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q2XgFC4t7yBjUuVDyVx1DQULNdzU5BIW
"""


from __future__ import absolute_import, division, print_function, unicode_literals

import math

import numpy as np
import torch
from torch import optim

import torch.nn as nn
import pickle
from tqdm import tqdm

from train_test_loaders import getLoader_CIFAR_LENET
from ratefunction import rate_function_single_value, eval_inverse_rate_at_lambda_signed, eval_rate_at_lambda_signed
from train_eval import get_loss_grad
from utils import list_files_in_directory
import argparse
from scipy.spatial import distance
import os
import sys

base_folder = "./"

"""Data"""


network = "Inception_SGD"
dataset_name = "CIFAR10"


parser = argparse.ArgumentParser()
parser.add_argument("-b", "--BATCH_SIZE", help="BATCH_SIZE", default= 50, type=int)

args = parser.parse_args()

n_models = 10
TRAIN_SIZE = 5000
TEST_SIZE = 0
BATCH_SIZE = args.BATCH_SIZE
# Hyper-Parameters
RANDOM_SEED = 2147483647
LEARNING_RATE = 0.01
N_ITERS = 1500
N_STEPS = 150
IMG_SIZE = 32
N_CLASSES = 10
WEIGHT_DECAY = 0.01

name_model=f"{network}_{dataset_name}_{TRAIN_SIZE}_{TEST_SIZE}_{BATCH_SIZE}"

# setup devices
torch.manual_seed(RANDOM_SEED)
if torch.cuda.is_available():
    device = torch.device("cuda:0")
    torch.cuda.manual_seed(RANDOM_SEED)
elif torch.backends.mps.is_available():
    device = torch.device("mps")
    torch.cuda.manual_seed(RANDOM_SEED)
else:
    device = torch.device("cpu")

"""# Train"""

train_loader_batch, _ = getLoader_CIFAR_LENET(TRAIN_SIZE,TEST_SIZE, BATCH_SIZE, BATCH_SIZE, RANDOM_SEED)
train_loader_full, test_loader_full = getLoader_CIFAR_LENET(TRAIN_SIZE,TEST_SIZE, BATCH_SIZE, BATCH_SIZE, RANDOM_SEED)

torch.manual_seed(RANDOM_SEED)


# Example usage
directory_path = f'./models/{name_model}/'
files = list_files_in_directory(directory_path)


g_cuda = torch.Generator(device='cpu')
g_cuda.manual_seed(RANDOM_SEED)


criterion = nn.CrossEntropyLoss()
criterion_nr = nn.CrossEntropyLoss(reduction="none")  # supervised classification loss

with open(directory_path + files[0], "rb") as handle:
    model = pickle.load(handle)

optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)

metrics = {
    'iter': [],
    'lamb_rate_D': [],
    'lamb_rate_B': [],
    'J_D': [],
    'J_B': [],
    'rate_D': [],
    'rate_B': [],
    'inv_D': [],
    'inv_D_approx': [],
    'inv_B': [],
    'inv_B_approx': [],
    'L': [],
    'Lhat': [],
    'Lbatch': [],
    'variance': [],
    'cosine_Iinv': [],
    'cosine_Alpha': [],
    'cosine_Lhat_Lbatch': [],
    'gradientNorm_L': [],
    'gradientNorm_Iinv': [],
    'gradientNorm_alpha': []
}

data_iter = iter(train_loader_batch)
iters_per_epoch = len(data_iter)
aux_loss = 1

tq = tqdm(range(N_ITERS))


for it in tq:

    # Get inputs and targets. If loader is exhausted, reinitialize.
    try:
        inputs, target = next(data_iter)
    except StopIteration:
        # StopIteration is thrown if dataset ends
        # reinitialize data loader
        data_iter = iter(train_loader_batch)
        inputs, target = next(data_iter)


    if it % N_STEPS == 0 and it != 0:
        with open(directory_path + f"model_{int(it)}.pickle", "rb") as handle:
            model = pickle.load(handle)
        optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)

        optimizer.zero_grad()
        # Forward pass
        losses = get_loss_grad(device, model, test_loader_full)
        L = torch.mean(losses)
        L.backward()

        gradients_L=[]
        for name, param in model.named_parameters():
            if param.grad is not None:
                gradients_L.append(param.grad.clone().flatten())

        gradients_L = torch.cat(gradients_L,dim=0).detach().cpu().numpy()

        metrics['gradientNorm_L'].append(np.linalg.norm(gradients_L))

        optimizer.zero_grad()
        Lhat = torch.mean(get_loss_grad(device, model, train_loader_full))
        Lhat.backward()

        gradients_Lhat=[]
        for name, param in model.named_parameters():
            if param.grad is not None:
                gradients_Lhat.append(param.grad.clone().flatten())

        gradients_Lhat = torch.cat(gradients_Lhat,dim=0).detach().cpu().numpy()

        optimizer.zero_grad()
        # Move data to device
        inputs = inputs.to(device)
        target = target.to(device)
        Lbatch = criterion(model(inputs), target)
        Lbatch.backward()

        gradients_Lbatch=[]
        for name, param in model.named_parameters():
            if param.grad is not None:
                gradients_Lbatch.append(param.grad.clone().flatten())

        gradients_Lbatch = torch.cat(gradients_Lbatch,dim=0).detach().cpu().numpy()
        cosine_Lhat_Lbatch=distance.cosine(gradients_Lhat, gradients_Lbatch)

        inputs = None
        target = None


        metrics['iter'].append(it)
        metrics['L'].append(L.detach().cpu().numpy())
        metrics['Lhat'].append(Lhat.detach().cpu().numpy())
        metrics['Lbatch'].append(Lbatch.detach().cpu().numpy())
        metrics['cosine_Lhat_Lbatch'].append(cosine_Lhat_Lbatch)

        with torch.no_grad():
            #Model.eval

            rate_val, lamb_rate, J_D = rate_function_single_value(losses, L - Lhat, device)
            metrics['rate_D'].append(rate_val)
            metrics['lamb_rate_D'].append(lamb_rate)
            metrics['J_D'].append(J_D)


            #inv_rate_val, lamb_inv, _ = inv_rate_function_single_value(model, losses, rate_val, device, test_loader)
            rate_val = torch.tensor(rate_val, dtype=torch.float32).to(device)
            lamb_rate = torch.tensor(lamb_rate, dtype=torch.float32).to(device)

            rate_val_batch, lamb_rate_batch, J_batch = rate_function_single_value(losses, L - Lbatch, device)
            metrics['rate_B'].append(rate_val_batch)
            metrics['lamb_rate_B'].append(lamb_rate_batch)
            metrics['J_B'].append(J_batch)

            #inv_rate_val_batch, lamb_inv_batch, _ = inv_rate_function_single_value(model,losses, rate_val_batch, device, test_loader)
            rate_val_batch = torch.tensor(rate_val_batch, dtype=torch.float32).to(device)
            lamb_rate_batch = torch.tensor(lamb_rate_batch, dtype=torch.float32).to(device)

        #Grad Iinv(theta,alpha_D)
        optimizer.zero_grad()
        losses = get_loss_grad(device, model, test_loader_full)
        inv_rate_f = eval_inverse_rate_at_lambda_signed(losses, lamb_rate.detach(), rate_val.detach(), device)

        inv_rate_f.backward()

        gradients_Iinv_D=[]
        for name, param in model.named_parameters():
            if param.grad is not None:
                gradients_Iinv_D.append(param.grad.clone().flatten())

        gradients_Iinv_D = torch.cat(gradients_Iinv_D,dim=0).detach().cpu().numpy()
        metrics['inv_D'].append(inv_rate_f.detach().cpu().numpy())



        #Grad Iinv(theta,alpha_batch)
        optimizer.zero_grad()
        losses = get_loss_grad(device, model, test_loader_full)
        inv_rate_f_batch = eval_inverse_rate_at_lambda_signed(losses, lamb_rate_batch.detach(), rate_val_batch.detach(), device)

        inv_rate_f_batch.backward()
        gradients_Iinv_batch=[]
        for name, param in model.named_parameters():
            if param.grad is not None:
                gradients_Iinv_batch.append(param.grad.clone().flatten())

        gradients_Iinv_batch = torch.cat(gradients_Iinv_batch,dim=0).detach().cpu().numpy()
        metrics['inv_B'].append(inv_rate_f_batch.detach().cpu().numpy())
        metrics['gradientNorm_Iinv'].append(np.linalg.norm(gradients_Iinv_batch))

        cosine_Iinv=distance.cosine(gradients_Iinv_D, gradients_Iinv_batch)
        metrics['cosine_Iinv'].append(cosine_Iinv)


        # #Grad alpha(theta,D)
        # optimizer.zero_grad()
        # losses = get_loss_grad(device, model, test_loader_full)
        # L = torch.mean(losses)
        # Lhat = torch.mean(get_loss_grad(device, model, train_loader_full))
        #
        # rate_f = inv_rate_f.detach()*eval_rate_at_lambda_signed(losses, lamb_rate, L-Lhat, device)
        # rate_f.backward()
        #
        # metrics['variance'].append(torch.var(losses).detach().cpu().numpy())
        # metrics['inv_D_approx'].append((torch.sign(rate_val)*torch.sqrt_(2*torch.abs(rate_val)*torch.var(losses))).detach().cpu().numpy())
        #
        # gradients_D=[]
        # for name, param in model.named_parameters():
        #     if param.grad is not None:
        #         gradients_D.append(param.grad.clone().flatten())
        #
        # gradients_D = torch.cat(gradients_D,dim=0)
        #
        # #Grad alpha(theta,batch)
        # optimizer.zero_grad()
        # losses = get_loss_grad(device, model, test_loader_full)
        # L = torch.mean(losses)
        # Lhat = torch.mean(get_loss_grad(device, model, train_loader_full))
        #
        # rate_f_batch = inv_rate_f_batch.detach()*eval_rate_at_lambda_signed(losses, lamb_rate_batch, L-Lbatch, device)
        # rate_f_batch.backward()
        #
        # metrics['inv_B_approx'].append((torch.sign(rate_val_batch)*torch.sqrt_(2*torch.abs(rate_val_batch)*torch.var(losses))).detach().cpu().numpy())
        #
        # gradients_batch=[]
        # for name, param in model.named_parameters():
        #     if param.grad is not None:
        #         gradients_batch.append(param.grad.clone().flatten())
        #
        # gradients_batch = torch.cat(gradients_batch,dim=0)
        #
        # metrics['gradientNorm_alpha'].append(torch.norm(gradients_batch).detach().cpu().numpy())
        #
        # cosine_Alpha=torch.nn.functional.cosine_similarity(gradients_D, gradients_batch,dim=0)
        # metrics['cosine_Alpha'].append(cosine_Alpha.detach().cpu().numpy())


with open(f'./FigGradientAligment/metrics/metrics_{name_model}.pickle', 'wb') as f:
    pickle.dump(metrics, f, pickle.HIGHEST_PROTOCOL)



