# -*- coding: utf-8 -*-
"""Abnormality SGD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q2XgFC4t7yBjUuVDyVx1DQULNdzU5BIW
"""


from __future__ import absolute_import, division, print_function, unicode_literals

from tqdm import tqdm
import numpy as np
import torch
import pickle

import argparse
from ratefunctiontorch import RateCumulant

base_folder = "./"

"""Data"""


dataset_name = "CIFAR10"




parser = argparse.ArgumentParser()
parser.add_argument("-m", "--MODEL", help="MODEL", default= "MLP", type=str)
parser.add_argument("-b", "--BATCH_SIZE", help="BATCH_SIZE", default= 2500, type=int)
parser.add_argument("-i", "--N_ITERS", help="N_ITERS", default= 3000, type=int)
parser.add_argument("-s", "--N_STEPS", help="N_STEPS", default= 30, type=int)
parser.add_argument("-w", "--WEIGHT_DECAY", help="WEIGHT_DECAY", default= 0.0, type=float)

args = parser.parse_args()

network = args.MODEL
TRAIN_SIZE = 0
TEST_SIZE = 0
BATCH_SIZE = args.BATCH_SIZE
# Hyper-Parameters
RANDOM_SEED = 2147483647
LEARNING_RATE = 0.01
N_ITERS = args.N_ITERS
N_STEPS = args.N_STEPS
IMG_SIZE = 32
N_CLASSES = 10
WEIGHT_DECAY = args.WEIGHT_DECAY

name_model=f"{network}_{dataset_name}_{TRAIN_SIZE}_{TEST_SIZE}_{BATCH_SIZE}_{WEIGHT_DECAY}"

# setup devices
torch.manual_seed(RANDOM_SEED)
if torch.cuda.is_available():
    device = torch.device("cuda:0")
    torch.cuda.manual_seed(RANDOM_SEED)
elif torch.backends.mps.is_available():
    device = torch.device("mps")
    torch.cuda.manual_seed(RANDOM_SEED)
else:
    device = torch.device("cpu")

"""# Train"""

torch.manual_seed(RANDOM_SEED)


# Example usage
directory_path = f'./models/{name_model}/'


g_cuda = torch.Generator(device='cpu')
g_cuda.manual_seed(RANDOM_SEED)


# Initialize metrics for moments and cumulants
metrics = {
    'iter': [],
    'L_train': [],
    'L_test': [],
    'var_train': [],
    'var_test': [],
    'mean_alpha': [],
    'var_alpha': [],
    'Iinv_alpha_mean': [],
}


tq = tqdm(range(N_ITERS))



for it in tq:

    if (it % N_STEPS == 0 and it != 0):
        test_losses = torch.from_numpy(np.loadtxt(f'losses/{name_model}/test_losses_{int(it)}.csv', delimiter=',').astype(np.float32)).to(device)
        train_losses = torch.from_numpy(np.loadtxt(f'losses/{name_model}/train_losses_{int(it)}.csv', delimiter=',').astype(np.float32)).to(device)

        train_cumulant = RateCumulant.from_losses(train_losses)
        test_cumulant = RateCumulant.from_losses(test_losses)

        L = test_cumulant.compute_mean()
        L_train = train_cumulant.compute_mean()

        # Split test_loss into batches
        num_batches = len(train_cumulant.get_losses()) // BATCH_SIZE
        train_loss_batches = torch.tensor_split(train_cumulant.get_losses(), num_batches)
    

        # Calculate alpha for each batch
        eval_points = []
        for losses_batch in train_loss_batches:
            eval_points.append( L-torch.mean(losses_batch))

        alpha_batches = test_cumulant.compute_rate_function(evaluation_points=eval_points)

        alpha_mean = torch.mean(alpha_batches)
        alpha_var = torch.var(alpha_batches)

        Iinv_alpha_mean = test_cumulant.compute_inverse_rate_function(alpha_mean)

        metrics['iter'].append(it)
        metrics['L_train'].append(L_train.detach().cpu().numpy())
        metrics['L_test'].append(L.detach().cpu().numpy())
        metrics['var_train'].append(train_cumulant.compute_variance().detach().cpu().numpy())
        metrics['var_test'].append(test_cumulant.compute_variance().detach().cpu().numpy())
        metrics['mean_alpha'].append(alpha_mean.detach().cpu().numpy())
        metrics['var_alpha'].append(alpha_var.detach().cpu().numpy())
        metrics['Iinv_alpha_mean'].append(Iinv_alpha_mean.detach().cpu().numpy())




with open(f'metrics/metrics_{name_model}.pickle', 'wb') as f:
    pickle.dump(metrics, f, pickle.HIGHEST_PROTOCOL)



